{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(len(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "x,y = iris.data, iris.target\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(5,), random_state=1,\n",
    "                    learning_rate='constant', learning_rate_init=0.01, max_iter=500, activation='logistic',\n",
    "                   momentum=0.9, verbose=True, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18799540\n",
      "Iteration 2, loss = 1.17023886\n",
      "Iteration 3, loss = 1.15364214\n",
      "Iteration 4, loss = 1.13829230\n",
      "Iteration 5, loss = 1.12426343\n",
      "Iteration 6, loss = 1.11161135\n",
      "Iteration 7, loss = 1.10035057\n",
      "Iteration 8, loss = 1.09041794\n",
      "Iteration 9, loss = 1.08164866\n",
      "Iteration 10, loss = 1.07379583\n",
      "Iteration 11, loss = 1.06658909\n",
      "Iteration 12, loss = 1.05978616\n",
      "Iteration 13, loss = 1.05319689\n",
      "Iteration 14, loss = 1.04668636\n",
      "Iteration 15, loss = 1.04016716\n",
      "Iteration 16, loss = 1.03358869\n",
      "Iteration 17, loss = 1.02692688\n",
      "Iteration 18, loss = 1.02017518\n",
      "Iteration 19, loss = 1.01333709\n",
      "Iteration 20, loss = 1.00642014\n",
      "Iteration 21, loss = 0.99943110\n",
      "Iteration 22, loss = 0.99237260\n",
      "Iteration 23, loss = 0.98524105\n",
      "Iteration 24, loss = 0.97802629\n",
      "Iteration 25, loss = 0.97071257\n",
      "Iteration 26, loss = 0.96328089\n",
      "Iteration 27, loss = 0.95571202\n",
      "Iteration 28, loss = 0.94798954\n",
      "Iteration 29, loss = 0.94010217\n",
      "Iteration 30, loss = 0.93204494\n",
      "Iteration 31, loss = 0.92381889\n",
      "Iteration 32, loss = 0.91542945\n",
      "Iteration 33, loss = 0.90688393\n",
      "Iteration 34, loss = 0.89818880\n",
      "Iteration 35, loss = 0.88934768\n",
      "Iteration 36, loss = 0.88036049\n",
      "Iteration 37, loss = 0.87122412\n",
      "Iteration 38, loss = 0.86193418\n",
      "Iteration 39, loss = 0.85248724\n",
      "Iteration 40, loss = 0.84288294\n",
      "Iteration 41, loss = 0.83312555\n",
      "Iteration 42, loss = 0.82322489\n",
      "Iteration 43, loss = 0.81319631\n",
      "Iteration 44, loss = 0.80306016\n",
      "Iteration 45, loss = 0.79284055\n",
      "Iteration 46, loss = 0.78256389\n",
      "Iteration 47, loss = 0.77225727\n",
      "Iteration 48, loss = 0.76194692\n",
      "Iteration 49, loss = 0.75165677\n",
      "Iteration 50, loss = 0.74140727\n",
      "Iteration 51, loss = 0.73121439\n",
      "Iteration 52, loss = 0.72108910\n",
      "Iteration 53, loss = 0.71103724\n",
      "Iteration 54, loss = 0.70106011\n",
      "Iteration 55, loss = 0.69115577\n",
      "Iteration 56, loss = 0.68132084\n",
      "Iteration 57, loss = 0.67155253\n",
      "Iteration 58, loss = 0.66185013\n",
      "Iteration 59, loss = 0.65221575\n",
      "Iteration 60, loss = 0.64265470\n",
      "Iteration 61, loss = 0.63317646\n",
      "Iteration 62, loss = 0.62379552\n",
      "Iteration 63, loss = 0.61453070\n",
      "Iteration 64, loss = 0.60540363\n",
      "Iteration 65, loss = 0.59643701\n",
      "Iteration 66, loss = 0.58765295\n",
      "Iteration 67, loss = 0.57907155\n",
      "Iteration 68, loss = 0.57070952\n",
      "Iteration 69, loss = 0.56257903\n",
      "Iteration 70, loss = 0.55468685\n",
      "Iteration 71, loss = 0.54703407\n",
      "Iteration 72, loss = 0.53961630\n",
      "Iteration 73, loss = 0.53242461\n",
      "Iteration 74, loss = 0.52544672\n",
      "Iteration 75, loss = 0.51866830\n",
      "Iteration 76, loss = 0.51207411\n",
      "Iteration 77, loss = 0.50564876\n",
      "Iteration 78, loss = 0.49937733\n",
      "Iteration 79, loss = 0.49324556\n",
      "Iteration 80, loss = 0.48723999\n",
      "Iteration 81, loss = 0.48134790\n",
      "Iteration 82, loss = 0.47555730\n",
      "Iteration 83, loss = 0.46985703\n",
      "Iteration 84, loss = 0.46423697\n",
      "Iteration 85, loss = 0.45868828\n",
      "Iteration 86, loss = 0.45320343\n",
      "Iteration 87, loss = 0.44777611\n",
      "Iteration 88, loss = 0.44240091\n",
      "Iteration 89, loss = 0.43707325\n",
      "Iteration 90, loss = 0.43178942\n",
      "Iteration 91, loss = 0.42654678\n",
      "Iteration 92, loss = 0.42134378\n",
      "Iteration 93, loss = 0.41617961\n",
      "Iteration 94, loss = 0.41105379\n",
      "Iteration 95, loss = 0.40596578\n",
      "Iteration 96, loss = 0.40091507\n",
      "Iteration 97, loss = 0.39590135\n",
      "Iteration 98, loss = 0.39092471\n",
      "Iteration 99, loss = 0.38598560\n",
      "Iteration 100, loss = 0.38108466\n",
      "Iteration 101, loss = 0.37622267\n",
      "Iteration 102, loss = 0.37140068\n",
      "Iteration 103, loss = 0.36662003\n",
      "Iteration 104, loss = 0.36188226\n",
      "Iteration 105, loss = 0.35718899\n",
      "Iteration 106, loss = 0.35254169\n",
      "Iteration 107, loss = 0.34794176\n",
      "Iteration 108, loss = 0.34339062\n",
      "Iteration 109, loss = 0.33888968\n",
      "Iteration 110, loss = 0.33444029\n",
      "Iteration 111, loss = 0.33004372\n",
      "Iteration 112, loss = 0.32570116\n",
      "Iteration 113, loss = 0.32141381\n",
      "Iteration 114, loss = 0.31718284\n",
      "Iteration 115, loss = 0.31300929\n",
      "Iteration 116, loss = 0.30889407\n",
      "Iteration 117, loss = 0.30483792\n",
      "Iteration 118, loss = 0.30084149\n",
      "Iteration 119, loss = 0.29690531\n",
      "Iteration 120, loss = 0.29302979\n",
      "Iteration 121, loss = 0.28921517\n",
      "Iteration 122, loss = 0.28546164\n",
      "Iteration 123, loss = 0.28176932\n",
      "Iteration 124, loss = 0.27813823\n",
      "Iteration 125, loss = 0.27456832\n",
      "Iteration 126, loss = 0.27105944\n",
      "Iteration 127, loss = 0.26761136\n",
      "Iteration 128, loss = 0.26422382\n",
      "Iteration 129, loss = 0.26089643\n",
      "Iteration 130, loss = 0.25762877\n",
      "Iteration 131, loss = 0.25442033\n",
      "Iteration 132, loss = 0.25127059\n",
      "Iteration 133, loss = 0.24817897\n",
      "Iteration 134, loss = 0.24514487\n",
      "Iteration 135, loss = 0.24216764\n",
      "Iteration 136, loss = 0.23924660\n",
      "Iteration 137, loss = 0.23638106\n",
      "Iteration 138, loss = 0.23357029\n",
      "Iteration 139, loss = 0.23081354\n",
      "Iteration 140, loss = 0.22811001\n",
      "Iteration 141, loss = 0.22545893\n",
      "Iteration 142, loss = 0.22285948\n",
      "Iteration 143, loss = 0.22031085\n",
      "Iteration 144, loss = 0.21781221\n",
      "Iteration 145, loss = 0.21536275\n",
      "Iteration 146, loss = 0.21296162\n",
      "Iteration 147, loss = 0.21060800\n",
      "Iteration 148, loss = 0.20830105\n",
      "Iteration 149, loss = 0.20603993\n",
      "Iteration 150, loss = 0.20382382\n",
      "Iteration 151, loss = 0.20165187\n",
      "Iteration 152, loss = 0.19952326\n",
      "Iteration 153, loss = 0.19743717\n",
      "Iteration 154, loss = 0.19539279\n",
      "Iteration 155, loss = 0.19338931\n",
      "Iteration 156, loss = 0.19142592\n",
      "Iteration 157, loss = 0.18950184\n",
      "Iteration 158, loss = 0.18761628\n",
      "Iteration 159, loss = 0.18576846\n",
      "Iteration 160, loss = 0.18395763\n",
      "Iteration 161, loss = 0.18218302\n",
      "Iteration 162, loss = 0.18044389\n",
      "Iteration 163, loss = 0.17873950\n",
      "Iteration 164, loss = 0.17706912\n",
      "Iteration 165, loss = 0.17543204\n",
      "Iteration 166, loss = 0.17382756\n",
      "Iteration 167, loss = 0.17225499\n",
      "Iteration 168, loss = 0.17071363\n",
      "Iteration 169, loss = 0.16920283\n",
      "Iteration 170, loss = 0.16772193\n",
      "Iteration 171, loss = 0.16627027\n",
      "Iteration 172, loss = 0.16484722\n",
      "Iteration 173, loss = 0.16345216\n",
      "Iteration 174, loss = 0.16208448\n",
      "Iteration 175, loss = 0.16074357\n",
      "Iteration 176, loss = 0.15942885\n",
      "Iteration 177, loss = 0.15813974\n",
      "Iteration 178, loss = 0.15687568\n",
      "Iteration 179, loss = 0.15563610\n",
      "Iteration 180, loss = 0.15442048\n",
      "Iteration 181, loss = 0.15322826\n",
      "Iteration 182, loss = 0.15205895\n",
      "Iteration 183, loss = 0.15091201\n",
      "Iteration 184, loss = 0.14978696\n",
      "Iteration 185, loss = 0.14868331\n",
      "Iteration 186, loss = 0.14760058\n",
      "Iteration 187, loss = 0.14653830\n",
      "Iteration 188, loss = 0.14549602\n",
      "Iteration 189, loss = 0.14447328\n",
      "Iteration 190, loss = 0.14346966\n",
      "Iteration 191, loss = 0.14248471\n",
      "Iteration 192, loss = 0.14151804\n",
      "Iteration 193, loss = 0.14056923\n",
      "Iteration 194, loss = 0.13963787\n",
      "Iteration 195, loss = 0.13872359\n",
      "Iteration 196, loss = 0.13782600\n",
      "Iteration 197, loss = 0.13694473\n",
      "Iteration 198, loss = 0.13607941\n",
      "Iteration 199, loss = 0.13522970\n",
      "Iteration 200, loss = 0.13439524\n",
      "Iteration 201, loss = 0.13357571\n",
      "Iteration 202, loss = 0.13277076\n",
      "Iteration 203, loss = 0.13198008\n",
      "Iteration 204, loss = 0.13120335\n",
      "Iteration 205, loss = 0.13044027\n",
      "Iteration 206, loss = 0.12969053\n",
      "Iteration 207, loss = 0.12895385\n",
      "Iteration 208, loss = 0.12822994\n",
      "Iteration 209, loss = 0.12751853\n",
      "Iteration 210, loss = 0.12681933\n",
      "Iteration 211, loss = 0.12613208\n",
      "Iteration 212, loss = 0.12545654\n",
      "Iteration 213, loss = 0.12479243\n",
      "Iteration 214, loss = 0.12413952\n",
      "Iteration 215, loss = 0.12349758\n",
      "Iteration 216, loss = 0.12286635\n",
      "Iteration 217, loss = 0.12224562\n",
      "Iteration 218, loss = 0.12163516\n",
      "Iteration 219, loss = 0.12103475\n",
      "Iteration 220, loss = 0.12044418\n",
      "Iteration 221, loss = 0.11986325\n",
      "Iteration 222, loss = 0.11929175\n",
      "Iteration 223, loss = 0.11872948\n",
      "Iteration 224, loss = 0.11817625\n",
      "Iteration 225, loss = 0.11763187\n",
      "Iteration 226, loss = 0.11709617\n",
      "Iteration 227, loss = 0.11656896\n",
      "Iteration 228, loss = 0.11605006\n",
      "Iteration 229, loss = 0.11553930\n",
      "Iteration 230, loss = 0.11503653\n",
      "Iteration 231, loss = 0.11454157\n",
      "Iteration 232, loss = 0.11405427\n",
      "Iteration 233, loss = 0.11357448\n",
      "Iteration 234, loss = 0.11310203\n",
      "Iteration 235, loss = 0.11263680\n",
      "Iteration 236, loss = 0.11217862\n",
      "Iteration 237, loss = 0.11172736\n",
      "Iteration 238, loss = 0.11128289\n",
      "Iteration 239, loss = 0.11084506\n",
      "Iteration 240, loss = 0.11041376\n",
      "Iteration 241, loss = 0.10998884\n",
      "Iteration 242, loss = 0.10957019\n",
      "Iteration 243, loss = 0.10915769\n",
      "Iteration 244, loss = 0.10875121\n",
      "Iteration 245, loss = 0.10835064\n",
      "Iteration 246, loss = 0.10795587\n",
      "Iteration 247, loss = 0.10756678\n",
      "Iteration 248, loss = 0.10718326\n",
      "Iteration 249, loss = 0.10680522\n",
      "Iteration 250, loss = 0.10643255\n",
      "Iteration 251, loss = 0.10606514\n",
      "Iteration 252, loss = 0.10570290\n",
      "Iteration 253, loss = 0.10534573\n",
      "Iteration 254, loss = 0.10499353\n",
      "Iteration 255, loss = 0.10464622\n",
      "Iteration 256, loss = 0.10430371\n",
      "Iteration 257, loss = 0.10396590\n",
      "Iteration 258, loss = 0.10363271\n",
      "Iteration 259, loss = 0.10330406\n",
      "Iteration 260, loss = 0.10297986\n",
      "Iteration 261, loss = 0.10266003\n",
      "Iteration 262, loss = 0.10234450\n",
      "Iteration 263, loss = 0.10203318\n",
      "Iteration 264, loss = 0.10172601\n",
      "Iteration 265, loss = 0.10142291\n",
      "Iteration 266, loss = 0.10112380\n",
      "Iteration 267, loss = 0.10082862\n",
      "Iteration 268, loss = 0.10053729\n",
      "Iteration 269, loss = 0.10024975\n",
      "Iteration 270, loss = 0.09996594\n",
      "Iteration 271, loss = 0.09968579\n",
      "Iteration 272, loss = 0.09940923\n",
      "Iteration 273, loss = 0.09913620\n",
      "Iteration 274, loss = 0.09886665\n",
      "Iteration 275, loss = 0.09860050\n",
      "Iteration 276, loss = 0.09833772\n",
      "Iteration 277, loss = 0.09807823\n",
      "Iteration 278, loss = 0.09782199\n",
      "Iteration 279, loss = 0.09756893\n",
      "Iteration 280, loss = 0.09731901\n",
      "Iteration 281, loss = 0.09707216\n",
      "Iteration 282, loss = 0.09682835\n",
      "Iteration 283, loss = 0.09658752\n",
      "Iteration 284, loss = 0.09634963\n",
      "Iteration 285, loss = 0.09611461\n",
      "Iteration 286, loss = 0.09588243\n",
      "Iteration 287, loss = 0.09565304\n",
      "Iteration 288, loss = 0.09542640\n",
      "Iteration 289, loss = 0.09520245\n",
      "Iteration 290, loss = 0.09498116\n",
      "Iteration 291, loss = 0.09476248\n",
      "Iteration 292, loss = 0.09454638\n",
      "Iteration 293, loss = 0.09433280\n",
      "Iteration 294, loss = 0.09412172\n",
      "Iteration 295, loss = 0.09391308\n",
      "Iteration 296, loss = 0.09370686\n",
      "Iteration 297, loss = 0.09350301\n",
      "Iteration 298, loss = 0.09330150\n",
      "Iteration 299, loss = 0.09310229\n",
      "Iteration 300, loss = 0.09290534\n",
      "Iteration 301, loss = 0.09271062\n",
      "Iteration 302, loss = 0.09251810\n",
      "Iteration 303, loss = 0.09232774\n",
      "Iteration 304, loss = 0.09213950\n",
      "Iteration 305, loss = 0.09195336\n",
      "Iteration 306, loss = 0.09176929\n",
      "Iteration 307, loss = 0.09158724\n",
      "Iteration 308, loss = 0.09140720\n",
      "Iteration 309, loss = 0.09122912\n",
      "Iteration 310, loss = 0.09105299\n",
      "Iteration 311, loss = 0.09087877\n",
      "Iteration 312, loss = 0.09070643\n",
      "Iteration 313, loss = 0.09053595\n",
      "Iteration 314, loss = 0.09036730\n",
      "Iteration 315, loss = 0.09020044\n",
      "Iteration 316, loss = 0.09003536\n",
      "Iteration 317, loss = 0.08987203\n",
      "Iteration 318, loss = 0.08971042\n",
      "Iteration 319, loss = 0.08955051\n",
      "Iteration 320, loss = 0.08939227\n",
      "Iteration 321, loss = 0.08923568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 322, loss = 0.08908071\n",
      "Iteration 323, loss = 0.08892734\n",
      "Iteration 324, loss = 0.08877555\n",
      "Iteration 325, loss = 0.08862532\n",
      "Iteration 326, loss = 0.08847662\n",
      "Iteration 327, loss = 0.08832942\n",
      "Iteration 328, loss = 0.08818372\n",
      "Iteration 329, loss = 0.08803949\n",
      "Iteration 330, loss = 0.08789670\n",
      "Iteration 331, loss = 0.08775534\n",
      "Iteration 332, loss = 0.08761539\n",
      "Iteration 333, loss = 0.08747683\n",
      "Iteration 334, loss = 0.08733963\n",
      "Iteration 335, loss = 0.08720378\n",
      "Iteration 336, loss = 0.08706926\n",
      "Iteration 337, loss = 0.08693606\n",
      "Iteration 338, loss = 0.08680414\n",
      "Iteration 339, loss = 0.08667351\n",
      "Iteration 340, loss = 0.08654413\n",
      "Iteration 341, loss = 0.08641599\n",
      "Iteration 342, loss = 0.08628907\n",
      "Iteration 343, loss = 0.08616337\n",
      "Iteration 344, loss = 0.08603885\n",
      "Iteration 345, loss = 0.08591551\n",
      "Iteration 346, loss = 0.08579332\n",
      "Iteration 347, loss = 0.08567228\n",
      "Iteration 348, loss = 0.08555237\n",
      "Iteration 349, loss = 0.08543357\n",
      "Iteration 350, loss = 0.08531587\n",
      "Iteration 351, loss = 0.08519925\n",
      "Iteration 352, loss = 0.08508370\n",
      "Iteration 353, loss = 0.08496921\n",
      "Iteration 354, loss = 0.08485575\n",
      "Iteration 355, loss = 0.08474333\n",
      "Iteration 356, loss = 0.08463191\n",
      "Iteration 357, loss = 0.08452150\n",
      "Iteration 358, loss = 0.08441207\n",
      "Iteration 359, loss = 0.08430362\n",
      "Iteration 360, loss = 0.08419613\n",
      "Iteration 361, loss = 0.08408958\n",
      "Iteration 362, loss = 0.08398398\n",
      "Iteration 363, loss = 0.08387929\n",
      "Iteration 364, loss = 0.08377552\n",
      "Iteration 365, loss = 0.08367266\n",
      "Iteration 366, loss = 0.08357068\n",
      "Iteration 367, loss = 0.08346957\n",
      "Iteration 368, loss = 0.08336934\n",
      "Iteration 369, loss = 0.08326996\n",
      "Iteration 370, loss = 0.08317142\n",
      "Iteration 371, loss = 0.08307372\n",
      "Iteration 372, loss = 0.08297685\n",
      "Iteration 373, loss = 0.08288078\n",
      "Iteration 374, loss = 0.08278552\n",
      "Iteration 375, loss = 0.08269105\n",
      "Iteration 376, loss = 0.08259736\n",
      "Iteration 377, loss = 0.08250445\n",
      "Iteration 378, loss = 0.08241230\n",
      "Iteration 379, loss = 0.08232090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "-------------------------\n",
      "Saídas:  [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "Saida desejada:  [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "-------------------------\n",
      "Score:  1.0\n",
      "Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_treino, x_test, y_treino, y_test = train_test_split(x,y, test_size=0.3, random_state=1)\n",
    "mlp.fit(x_treino, y_treino)\n",
    "saidas = mlp.predict(x_test)\n",
    "print('-------------------------')\n",
    "\n",
    "print('Saídas: ', saidas)\n",
    "print('Saida desejada: ', y_test)\n",
    "\n",
    "print('-------------------------')\n",
    "\n",
    "print('Score: ', (saidas == y_test).sum() /len(x_test))\n",
    "print('Score: ', mlp.score(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
